# Turing Challenge - Chatbot RAG + Ejecuci칩n de C칩digo + Im치genes

Este proyecto es un sistema fullstack de chatbot con memoria din치mica, RAG (Retrieval-Augmented Generation), ejecuci칩n de c칩digo Python bajo demanda y soporte para im치genes extra칤das de PDFs.

## Requisitos
- Docker y Docker Compose (recomendado)
- Python 3.11+ (si quieres correr el backend sin Docker)
- Node.js 18+ (si quieres correr el frontend sin Docker)

## Estructura del proyecto
```
Turing_Challenge/
  back/           # Backend FastAPI (RAG, memoria, ejecuci칩n de c칩digo, im치genes)
    data/         # PDFs fuente
    images/       # Im치genes extra칤das de los PDFs (se genera en la ingesta)
    faiss_index/  # Base vectorial FAISS (se genera en la ingesta)
    service/      # Routers y l칩gica modular
    ingest.py     # Script de ingesta y extracci칩n
    main.py       # Entry point FastAPI
    requirements.txt
  front/          # Frontend React
    src/components/Chatbot.js
    ...
  config.yaml     # Configuraci칩n de credenciales y par치metros
  docker-compose.yml
  README.md
```

## Primer uso: instalaci칩n y arranque

### 1. Clona el repositorio y entra en la carpeta
```bash
git clone <repo-url>
cd Turing_Challenge
```

### 2. Coloca tus PDFs en `back/data/`
- Pon los archivos PDF que quieras indexar en la carpeta `back/data/`.

### 3. Configura credenciales y par치metros
- Edita `config.yaml` para poner tu API key de Groq, modelo, umbrales, etc.

### 4. (Opcional) Instala dependencias manualmente
Si no usas Docker:
```bash
cd back
pip install -r requirements.txt
cd ../front
npm install
```

### 5. Arranca todo con Docker Compose
```bash
docker compose up --build
```
- El primer arranque puede tardar (ingesta de PDFs, creaci칩n de FAISS, extracci칩n de im치genes).
- Si quieres forzar la ingesta de nuevo (por ejemplo, tras cambiar los PDFs), borra `back/faiss_index/` y `back/images/` antes de arrancar.

### 6. Accede al frontend
- Abre [http://localhost:3000](http://localhost:3000) en tu navegador.

## 쮺칩mo funciona?
- El chatbot responde usando un modelo LLM (Groq/Llama3) y, si la pregunta es relevante para los PDFs, usa RAG para dar contexto documental.
- Si la pregunta requiere c칩digo Python, el modelo lo genera y el backend lo ejecuta de forma segura, devolviendo el resultado.
- Si el contexto relevante tiene una imagen asociada (extra칤da del PDF), el backend la devuelve y el frontend la muestra.
- Puedes preguntar por im치genes por contexto o por nombre de archivo (ej: "dame la imagen 400_square_degrees_cluster.pdf_p2_0").
- El sistema tiene memoria din치mica: cuando el historial supera el umbral de tokens, se resume autom치ticamente.

## Ejemplos de uso
- "쯈u칠 dice el PDF sobre clusters de galaxias?"
- "쯇uedes mostrarme la imagen de la p치gina 2 del PDF sobre clusters de estrellas?"
- "Dame la imagen 400_square_degrees_cluster.pdf_p2_0"
- "Calcula la suma de los primeros 10 n칰meros naturales con c칩digo Python"

## Notas
- Las carpetas `back/images/` y `back/faiss_index/` est치n en el `.gitignore` y no se suben al repo.
- Si tienes problemas con la ingesta o las im치genes, revisa los logs del backend para errores.
- El frontend muestra emojis de transparencia (游닄 para RAG, 游눫 para solo modelo) y de "pensando" (游뱂).

## Cr칠ditos
- Basado en FastAPI, LangChain, FAISS, HuggingFace, React, Groq Cloud.
